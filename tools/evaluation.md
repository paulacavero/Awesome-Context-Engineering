

### [Promptfoo](https://github.com/promptfoo/promptfoo) â€“ Prompt evaluation framework  
Compare prompt and context variants with metrics, A/B testing, and model scoring.  
ðŸ’¡ Ideal for testing context effectiveness and optimizing prompt strategies. OSS (MIT).


### [BetterPrompt](https://github.com/stjordanis/betterprompt) â€“ Prompt optimization via evaluation  
Evaluates multiple prompts to identify the best performer using human or automated scoring.  
ðŸ’¡ Useful for optimizing instructions and testing context effectiveness. OSS (MIT).


### [Guardrails AI](https://github.com/guardrails-ai/guardrails) â€“ Output validation for LLMs  
Define rules to validate, fix or reject model outputs using YAML specs. Ensures structure, constraints, and format integrity.  
ðŸ’¡ Great for enforcing safe and correct outputs in context-sensitive flows. OSS (Apache 2.0).


### [adversarialâ€‘prompts](https://github.com/hwchase17/adversarial-prompts) â€“ Curated adversarial prompts  
A collection of prompts that succeed in bypassing or breaking LLM guardrails (evasion, injection, context leakage), with examples and defenses.  
ðŸ’¡ Useful for testing model robustness and evaluating context safety. OSS (MIT).
