
### [LM Studio](https://lmstudio.ai/) â€“ Local server + function-calling environment  
Run LLMs locally (LLaMA, Qwen, Gemmaâ€¦) with OpenAIâ€‘style REST API, tool use, RAG, and context window control. Includes CLI and SDK (OSS), GUI client for desktops.  
ðŸ’¡ Great for experimenting with tool integration and self-hosted LLM pipelines. Free to use; core SDK is MIT.


### [Ollama](https://github.com/ollama/ollama) â€“ Local LLM runtime with tool calling  
Run LLMs (Llamaâ€¯3, Qwen, Gemmaâ€¦) locally with an OpenAI-compatible API, CLI, and Python library. Supports function-calling tools on-premises.  
ðŸ’¡ Ideal for selfâ€‘hosted tool integration and offline LLM workflows. OSS (MIT).
